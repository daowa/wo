{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "## chinese decode \n",
    "def cn(s):\n",
    "#     return s.decode('utf8')\n",
    "    return s\n",
    "## 字符串类型转换为类别的数字编码\n",
    "dict_short_url  = {'init':0} #和短地址一样，统一记录使用过的int\n",
    "dict_short_url_index = {0: 'init'}\n",
    "def getShortUrl(s):\n",
    "    # 未记录，则添加记录\n",
    "    if dict_short_url.has_key(s) == False:\n",
    "        dict_short_url[s] = max(dict_short_url.values())+1\n",
    "        dict_short_url_index[dict_short_url[s]] = s\n",
    "    return dict_short_url[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# trainData means rawdata, X_train means cross_validation data\n",
    "# trainData = pd.merge(pd.read_csv(cn('E:\\\\work\\\\联通+旅游\\\\data\\\\1\\\\train_x.csv'),encoding=\"gb2312\"), pd.read_csv(cn('E:\\\\work\\\\联通+旅游\\\\data\\\\1\\\\train_y.csv'),encoding=\"gb2312\"), on=cn('用户标识'))\n",
    "trainData = pd.merge(pd.read_csv('/data/topic1/train_x.csv'), pd.read_csv('/data/topic1/train_y.csv'), on='用户标识')\n",
    "print 'march_load_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入外部特征\n",
    "## app点击数\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countapppv.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countapp.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countoutprovince.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countwebpv.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countweb.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countgroupticket'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countgrouptraffic'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countgroupfff'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countgrouphotel'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_countgroupweather'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_cluster.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_outprovince.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_topweb.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_webcluster.csv'), on='用户标识')\n",
    "trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_maxgroup.csv'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_groupticket'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_grouptraffic'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_groupfff'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_grouphotel'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_groupweather'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_hashweb.csv'), on='用户标识')\n",
    "# trainData = pd.merge(trainData, pd.read_csv('/data/topic1/feature_hashapp.csv'), on='用户标识')\n",
    "print 'march_external_feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去掉app特征\n",
    "# trainData.drop(trainData.columns[8:308], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # app二值化\n",
    "# for i in trainData.columns[8:308]:\n",
    "#     print i\n",
    "#     trainData['_' + i] = trainData[i].apply(lambda x: int(x>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apppv / count_app_pv，得到用户对app的偏好(舍去)\n",
    "# for i in trainData.columns[8:308]:\n",
    "#     print i\n",
    "#     trainData[i] = 1000 * trainData[i] / (trainData['count_app_pv']+1)#避免除以0的情况 #*1000是为了减少决策树分箱颗粒过大的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfidf\n",
    "# ## 获取每个词的idf\n",
    "# dict_idf = {}\n",
    "# for i in trainData.columns[8:308]:\n",
    "#     idf = 1 / float(len(trainData[trainData[i]>0])+1)\n",
    "#     dict_idf[i] = idf\n",
    "#     print i + ':' + str(idf)\n",
    "# for i in trainData.columns[8:308]:\n",
    "#     print i\n",
    "#     trainData[i] = 1000 * trainData[i] * dict_idf[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData['phone_brand'] = trainData[cn('手机品牌')].apply(getShortUrl)\n",
    "trainData['phone_model_number'] = trainData[cn('手机终端型号')].apply(getShortUrl)\n",
    "trainData['if_cross_province'] = trainData[cn('是否有跨省行为')].apply(getShortUrl)\n",
    "trainData['if_cross_conuntry'] = trainData[cn('是否有出境行为')].apply(getShortUrl)\n",
    "trainData['topweb'] = trainData['topweb'].apply(getShortUrl)\n",
    "trainData.drop([cn('手机品牌'), cn('是否有跨省行为'), cn('是否有出境行为'), cn('漫入省份'), cn('漫出省份'), cn('手机终端型号')], axis=1, inplace=True)\n",
    "print 'march_inner_feature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainData.drop([cn('用户标识'),cn('是否去过迪士尼')],axis=1), trainData[cn('是否去过迪士尼')], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一阶特征，lgb做分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改列名， lgb不支持中文列名\n",
    "dict_new_columns = {}\n",
    "for i in X_train.columns:\n",
    "    dict_new_columns[i] = str(getShortUrl(i))\n",
    "X_train.rename(columns=dict_new_columns, inplace = True)\n",
    "X_test.rename(columns=dict_new_columns, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 声明类别变量\n",
    "temp_categorical_feature = [cn('性别'),cn('大致消费水平'),'phone_brand','phone_model_number','if_cross_province','if_cross_conuntry',\n",
    "                           'hainan','shanghai','neimenggu','xizang','gansu','henan','cluster','webcluster','topweb','maxgroup']\n",
    "list_categorical_feature = [str(dict_short_url[item]) for item in temp_categorical_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train.values, label=y_train, feature_name=list(X_train.columns), categorical_feature=list_categorical_feature)\n",
    "test_data = lgb.Dataset(X_test.values, label=y_test, feature_name=list(X_test.columns), categorical_feature=list_categorical_feature)\n",
    "param = {'application': 'binary', 'boosting': 'dart', 'num_leaves':127, 'num_trees':1000, 'objective':'binary', 'min_child_samples':400, 'max_bin': 250, 'max_depth':-1, \n",
    "         'feature_fraction':1, 'metric':'auc','subsample':0.9, 'lambda_l1':1.5, 'lambda_l2':20, 'min_data_in_bin':100,\n",
    "        'bagging_fraction':1, 'bagging_freq':1, 'learning_rate':0.05}\n",
    "# param = {'application': 'binary', 'boosting': 'gbdt', 'num_leaves':155, 'num_trees':2000, 'objective':'binary', 'min_child_samples':400, 'max_bin': 250, 'max_depth':-1, \n",
    "#          'feature_fraction':1, 'metric':'auc','subsample':0.9, 'lambda_l1':1.5, 'lambda_l2':20, 'min_data_in_bin':100,\n",
    "#         'bagging_fraction':1, 'bagging_freq':10, 'learning_rate':0.05}\n",
    "# param = {'application': 'binary', 'boosting': 'gbdt', 'num_leaves':127, 'num_trees':1000, 'objective':'binary', 'min_child_samples':400, 'max_bin': 250, 'max_depth':-1, \n",
    "#          'feature_fraction':1, 'metric':'auc','subsample':0.9, 'lambda_l1':1.5, 'lambda_l2':20, 'min_data_in_bin':100,\n",
    "#         'bagging_fraction':1, 'bagging_freq':1, 'learning_rate':0.025,'is_unbalance':True}\n",
    "# bst=lgb.cv(param,train_data,num_boost_round=350,nfold=5)\n",
    "estimators = lgb.train(param,train_data,valid_sets=test_data,num_boost_round=800)\n",
    "# print len(bst['auc-mean'])\n",
    "print 'march_training_done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "train_data = xgb.DMatrix(X_train.values, label=y_train)\n",
    "test_data = xgb.DMatrix(X_test.values, label=y_test)\n",
    "watch_list = [(test_data, 'eval'), (train_data, 'train')]\n",
    "param = {'max_depth': 8, 'max_leaf_nodes':127, 'eta': 0.3, 'silent': 1, 'objective': 'rank:pairwise', 'max_leaf_nodes':1, 'subsample':0.9,\n",
    "        'min_child_weight': 1, 'alpha':0, 'lambda':1, 'gamma':0, 'scale_pos_weight':0.1, 'eval_metric':'auc'}\n",
    "bst = xgb.train(param, train_data, evals=watch_list, num_boost_round=100, early_stopping_rounds=100)\n",
    "ypred = bst.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估正确率\n",
    "from sklearn import metrics\n",
    "print metrics.classification_report(y_test, ypred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估AUC\n",
    "ypred = estimators.predict(X_test.values)\n",
    "auc = roc_auc_score(y_test, ypred)\n",
    "print 'AUC:',auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 评估AUC(训练数据)\n",
    "ypred = estimators.predict(X_train.values)\n",
    "print roc_auc_score(y_train, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance = pd.DataFrame({'feature':trainData.drop([cn('用户标识'),cn('是否去过迪士尼')],axis=1).columns, 'importance':bst.feature_importance()})\n",
    "# importance = importance.sort_values(by='importance', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
